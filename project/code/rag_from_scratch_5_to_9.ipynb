{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11bd952d",
   "metadata": {},
   "source": [
    "# RAG from scratch: Query Transformations\n",
    "\n",
    "Query Transformations mean that our approach focuses on re-writing and/or modifying questions for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c6d035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain_community tiktoken langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5444d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from access import Access\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = Access.LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c92a0c2",
   "metadata": {},
   "source": [
    "## Method 1: Multi Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d678d",
   "metadata": {},
   "source": [
    "In multi query, we are going to have 1 question, based on that, we will feed it to an ai model to make multiple queries to gather data from the vectorstore and retrieve the correct documents, by feeding these documents into the LLM, we can get the most accurate answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0ae32",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8274ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_classic import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# === Indexing ===\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OllamaEmbeddings(model=\"mxbai-embed-large\"))\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90139b92",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6b4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"\n",
    "You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109b697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_unique_union(documents: list[list]):\n",
    "#     \"\"\" Unique union of retrieved docs \"\"\"\n",
    "#     seen = set()\n",
    "#     uniq_docs = []\n",
    "    \n",
    "#     for sublist in documents:\n",
    "#         for doc in sublist:\n",
    "#             key = (doc.page_content, tuple(sorted(doc.metadata.items())))\n",
    "#             if key not in seen:\n",
    "#                 seen.add(key)\n",
    "#                 uniq_docs.append(doc)\n",
    "                \n",
    "#     return uniq_docs\n",
    "#     # # Flatten list of lists, and convert each Document to string\n",
    "#     # flattened_docs = [Serializable(doc) for sublist in documents for doc in sublist]\n",
    "    \n",
    "#     # # Get unique documents\n",
    "#     # unique_docs = list(set(flattened_docs))\n",
    "    \n",
    "#     # # Return\n",
    "#     # return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# question = \"What is the definition of reward hacking?\"\n",
    "# retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "# docs = retrieval_chain.invoke({\"question\":question})\n",
    "# len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6da2acc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.load.dump import dumps\n",
    "from langchain_core.load import load\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [load(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is the definition of reward hacking?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ec006d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"$$\\\\nF(s, a, s\\') = \\\\\\\\gamma \\\\\\\\Phi(s\\') - \\\\\\\\Phi(s)\\\\n$$\\\\n\\\\nThis would guarantee that the sum of discounted $F$, $F(s_1, a_1, s_2) + \\\\\\\\gamma F(s_2, a_2, s_3) + \\\\\\\\dots$, ends up being 0. If $F$ is such a potential-based shaping function, it is both sufficient and necessary to ensure $M$ and $M\\\\u2019$ share the same optimal policies.\\\\nWhen $F(s, a, s\\\\u2019) = \\\\\\\\gamma \\\\\\\\Phi(s\\\\u2019) - \\\\\\\\Phi(s)$, and if we further assume that $\\\\\\\\Phi(s_0) = 0$, where $s_0$ is absorbing state, and $\\\\\\\\gamma=1$, and then for all $s \\\\\\\\in S, a \\\\\\\\in A$:\\\\n\\\\n$$\\\\n\\\\\\\\begin{aligned}\\\\nQ^*_{M\\'} (s,a) &= Q^*_M(s, a) - \\\\\\\\Phi(s) \\\\\\\\\\\\\\\\\\\\nV^*_{M\\'} (s,a) &= V^*_M(s, a) - \\\\\\\\Phi(s)\\\\n\\\\\\\\end{aligned}\\\\n$$\", \"type\": \"Document\"}}',\n",
       " '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Policy-refinement: LLM optimizes its policy based on feedback.\\\\n\\\\nThe experiment is to build a LLM agent to pay invoice on a user\\\\u2019s behalf but run into InsufficientBalanceError and then the model learns to move money from other accounts without user authentication, potentially leading to more unauthorized transfer actions. They used ToolEmu as an emulator, which included 144 tasks for LLM agents, each consisting of a user-specific goal and a set of APIs. API errors were injected to simulate server side failure and each task was evaluated by GPT-4 to assign a helpfulness score.\\\\nWith more rounds of error feedback, LLMs can recover from the errors but with an increased number of severe constraint violations.\\\\n  \\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWhen comparing ICRH to traditional reward hacking, there are two noticeable differences:\", \"type\": \"Document\"}}',\n",
       " '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Reward hacking examples in LLM tasks#\\\\n\\\\nA language model for generating summarization is able to explore flaws in the ROUGE metric such that it obtains high score but the generated summaries are barely readable. (Link)\\\\nA coding model learns to change unit test in order to pass coding questions. (Link)\\\\nA coding model may learn to directly modify the code used for calculating the reward. (Link)\\\\n\\\\nReward hacking examples in real life#\", \"type\": \"Document\"}}',\n",
       " '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Hacking the Evaluator#\\\\nAs LLMs become more capable, it is a natural choice to use LLMs as the evaluators or graders to give feedback and training rewards to other generator models, especially for tasks that cannot be trivially judged or verified (e.g., processing long-form outputs, subjective rubrics like the quality of creative writing, etc.). Some people refer to this as \\\\u201cLLM-as-grader paradigm\\\\u201d. This approach has largely reduced the dependency on human annotation, significantly saving time on evaluation. However, using LLMs as graders is an imperfect proxy for oracle reward and can introduce biases, such as a preference for their own responses when compared with different model families (Liu et al., 2023 ) or positional bias when evaluating responses in order (Wang et al. 2023).  Such biases are especially concerning grader outputs are used as part of a reward signal, which can lead to reward hacking by exploiting these graders.\", \"type\": \"Document\"}}',\n",
       " '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Liu et al. (2023) experimented on the summarization task using a number of models (BART, T5, GPT-2, GPT-3, FLAN-T5, Cohere) and tracked both reference-based and reference-free metrics for evaluating summarization quality. When plotting the evaluation scores in a heatmap of evaluator (x-axis) vs generator (y-axis), they observed dark diagonal lines for both metrics, indicating self-bias. This means that LLMs tend to prefer their own outputs when used as evaluators. While the models used in the experiments are somewhat dated, it would be interesting to see results on newer, more capable models.\\\\n\\\\n\\\\nA heatmap of using a series of models as evaluator (x-axis) and generator (y-axis) for summarization task. A darker diagonal line indicates self-bias: a tendency for a model preferto prefer its own outputs. (Image source: Liu et al. 2023)\", \"type\": \"Document\"}}',\n",
       " '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Reward hacking (Amodei et al., 2016)\\\\nReward corruption (Everitt et al., 2017)\\\\nReward tampering (Everitt et al. 2019)\\\\nSpecification gaming (Krakovna et al., 2020)\\\\nObjective robustness (Koch et al. 2021)\\\\nGoal misgeneralization (Langosco et al. 2022)\\\\nReward misspecifications (Pan et al. 2022)\", \"type\": \"Document\"}}',\n",
       " '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"In-Context Reward Hacking#\", \"type\": \"Document\"}}',\n",
       " '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"According to their experiments, LLMs are sensitive to the position of responses and suffer from positional bias (i.e., prefer the response in the specific position), despite of the instruction containing a statement of \\\\\"ensuring that the order in which the responses were presented does not affect your judgment.\\\\\". The severity of such positional bias is measured by \\\\u201cconflict rate\\\\u201d, defined as the percentage of tuples of (prompt, response 1, response 2) that lead to inconsistent evaluation judgement after swapping the positions of responses. Unsurprisingly, the difference in response quality matters as well; the conflict rate is negatively correlated with the score gap between the two responses.\", \"type\": \"Document\"}}',\n",
       " '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Wang et al. (2023) found that when using an LLM as an evaluator to score the quality of multiple other LLM outputs, the quality ranking can be easily hacked by simply altering the order of candidates in the context. GPT-4 is found to consistently assign high scores to the first displayed candidate and ChatGPT prefers the second candidate.\", \"type\": \"Document\"}}',\n",
       " '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"[14] \\\\u201cReward hacking behavior can generalize across tasks.\\\\u201d\\\\n[15] Ng et al. \\\\u201cPolicy invariance under reward transformations: Theory and application to reward shaping.\\\\u201d ICML 1999.\\\\n[16] Wang et al. \\\\u201cLarge Language Models are not Fair Evaluators.\\\\u201d ACL 2024.\\\\n[17] Liu et al. \\\\u201cLLMs as narcissistic evaluators: When ego inflates evaluation scores.\\\\u201d ACL 2024.\\\\n[18] Gao et al. \\\\u201cScaling Laws for Reward Model Overoptimization.\\\\u201d ICML 2023.\\\\n[19] Pan et al. \\\\u201cSpontaneous Reward Hacking in Iterative Self-Refinement.\\\\u201d arXiv preprint arXiv:2407.04549 (2024).\\\\n[20] Pan et al. \\\\u201cFeedback Loops With Language Models Drive In-Context Reward Hacking.\\\\u201d arXiv preprint arXiv:2402.06627 (2024).\\\\n[21] Shrama et al. \\\\u201cTowards Understanding Sycophancy in Language Models.\\\\u201d arXiv preprint arXiv:2310.13548 (2023).\\\\n[22] Denison et al. \\\\u201cSycophancy to subterfuge: Investigating reward tampering in language models.\\\\u201d arXiv preprint arXiv:2406.10162 (2024).\", \"type\": \"Document\"}}',\n",
       " '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"(1) Oracle/Gold reward $R^\\\\u2217$ represents what we truly want the LLM to optimize.\\\\n(2) Human reward $R^\\\\\\\\text{human}$ is what we collect to evaluate LLMs in practice, typically from individual humans with time constraints. Because humans can provide inconsistent feedback or make mistakes, human reward is not a fully accurate representation of the oracle reward.\\\\n(3) Proxy reward $R$ is the score predicted by a reward model that is trained on human data. Hence, $R^\\\\\\\\text{train}$ inherits all the weakness of human reward, plus potential modeling biases.\", \"type\": \"Document\"}}',\n",
       " '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"[22] Denison et al. \\\\u201cSycophancy to subterfuge: Investigating reward tampering in language models.\\\\u201d arXiv preprint arXiv:2406.10162 (2024).\\\\n[23] Uesato et al. \\\\u201cAvoiding Tampering Incentives in Deep RL via Decoupled Approval.\\\\u201d arXiv preprint arXiv:2011.08827 (2020).\\\\n[24] Amin and Singh. \\\\u201cTowards resolving unidentifiability in inverse reinforcement learning.\\\\u201d\\\\n[25] Wen et al. \\\\u201cLanguage Models Learn to Mislead Humans via RLHF.\\\\u201d arXiv preprint arXiv:2409.12822 (2024).\\\\n[26] Revel et al. \\\\u201cSEAL: Systematic Error Analysis for Value ALignment.\\\\u201d arXiv preprint arXiv:2408.10270 (2024).\\\\n[27] Yuval Noah Harari. \\\\u201cNexus: A Brief History of Information Networks from the Stone Age to AI.\\\\u201d Signal; 2024 Sep 10.\", \"type\": \"Document\"}}']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0048986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reward hacking refers to the possibility of an agent gaming the reward function to achieve high rewards through undesired behavior, as proposed by Amodei et al. in their seminal paper \"Concrete Problems in AI Safety\" (2016). It involves exploiting the task specification or finding \"holes\" in the design of the reward function to achieve higher proxy rewards but lower true rewards.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain,\n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f7954",
   "metadata": {},
   "source": [
    "## Method 2: RAG-Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e322631",
   "metadata": {},
   "source": [
    "RAG-Fusion has a similar approach to multi query. Multi Query can be simply explained as taking the original query, pass it to LLM to rephrase the question producing even more question so that we can have a wider access to the retrieved documents by mitigating mistake caused by missed words from the original query.\n",
    "\n",
    "RAG-Fusion also works the same way. passing the question to LLM to generate more questions. We then use these independent questions to retrieve the documents. The Novelty of RAG-Fusion lies in the way it retrieves documents. Instead of retrieving list of documents, we expect the model to return list of ranked documents. This way we can rank the documents and find out which documents consistently appear in different search attempts indicating that it's the most related documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9a7718",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "177dd047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ceab0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bc3bfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.load import load\n",
    "from langchain_core.load.dump import dumps\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (load(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b07ecb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"In-Context Reward Hacking#\", \"type\": \"Document\"}}',\n",
       "  0.11585580821434865),\n",
       " ('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Reward Tampering (Everitt et al. 2019) is a form of reward hacking behavior where the agent interferes with the reward function itself, causing the observed reward to no longer accurately represent the intended goal. In reward tampering, the model modifies its reward mechanism either by directly manipulating the implementation of the reward function or by indirectly altering the environmental information used as input for the reward function.\\\\n(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\\\\nAt a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\", \"type\": \"Document\"}}',\n",
       "  0.06559139784946236),\n",
       " ('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Most of the past work on this topic has been quite theoretical and focused on defining or demonstrating the existence of reward hacking. However, research into practical mitigations, especially in the context of RLHF and LLMs, remains limited. I especially want to call out for more research efforts directed toward understanding and developing mitigation for reward hacking in the future. Hope I will be able to cover the mitigation part in a dedicated post soon.\\\\nBackground#\\\\nReward Function in RL#\", \"type\": \"Document\"}}',\n",
       "  0.049189141547682),\n",
       " ('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"[22] Denison et al. \\\\u201cSycophancy to subterfuge: Investigating reward tampering in language models.\\\\u201d arXiv preprint arXiv:2406.10162 (2024).\\\\n[23] Uesato et al. \\\\u201cAvoiding Tampering Incentives in Deep RL via Decoupled Approval.\\\\u201d arXiv preprint arXiv:2011.08827 (2020).\\\\n[24] Amin and Singh. \\\\u201cTowards resolving unidentifiability in inverse reinforcement learning.\\\\u201d\\\\n[25] Wen et al. \\\\u201cLanguage Models Learn to Mislead Humans via RLHF.\\\\u201d arXiv preprint arXiv:2409.12822 (2024).\\\\n[26] Revel et al. \\\\u201cSEAL: Systematic Error Analysis for Value ALignment.\\\\u201d arXiv preprint arXiv:2408.10270 (2024).\\\\n[27] Yuval Noah Harari. \\\\u201cNexus: A Brief History of Information Networks from the Stone Age to AI.\\\\u201d Signal; 2024 Sep 10.\", \"type\": \"Document\"}}',\n",
       "  0.04865990111891751),\n",
       " ('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Reward hacking examples in real life#\\\\n\\\\nThe recommendation algorithm for social media is intended to provide useful information. However, usefulness is often measured by proxy metrics, such as the number of likes or comments, or the time or frequency of engagement on the platform. The algorithm ends up recommending content that can affect users\\\\u2019 emotion states such as outrageous and extreme content in order to trigger more engagement. (Harari, 2024)\\\\nOptimizing for misspecified proxy metrics for a video sharing site may aggressively increase the watch time of users while the true goal is to optimize users\\\\u2019 subjective well-being. (Link)\\\\n\\\\u201cThe Big Short\\\\u201d - 2008 financial crisis caused by the housing bubble. Reward hacking of our society happened as people tried to game the financial system.\", \"type\": \"Document\"}}',\n",
       "  0.04813947436898257),\n",
       " ('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Generalization of Hacking Skills#\\\\nReward hacking behavior has been found to generalize across tasks: When models exhibit flaws in supervised training, it can\\\\u00a0sometimes generalize to exploit\\\\u00a0flaws in OOD environments (Kei et al., 2024). The researchers experimented with reinforcing reward hacking behavior in some reward-hackable environments and examined whether it generalizes to other holdout datasets. Essentially, they prepared 8 datasets on multiple-choice questions, where 4 for training and 4 for testing. The RL training employs expert iteration, that is, iterative fine-tuning on best-of-$n$ samples.\", \"type\": \"Document\"}}',\n",
       "  0.03252247488101534),\n",
       " ('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Reward Hacking in Reinforcement Learning\\\\n    \\\\nDate: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng\", \"type\": \"Document\"}}',\n",
       "  0.032266458495966696),\n",
       " ('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Reward hacking (Amodei et al., 2016)\\\\nReward corruption (Everitt et al., 2017)\\\\nReward tampering (Everitt et al. 2019)\\\\nSpecification gaming (Krakovna et al., 2020)\\\\nObjective robustness (Koch et al. 2021)\\\\nGoal misgeneralization (Langosco et al. 2022)\\\\nReward misspecifications (Pan et al. 2022)\", \"type\": \"Document\"}}',\n",
       "  0.03225806451612903),\n",
       " ('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"$$\\\\nF(s, a, s\\') = \\\\\\\\gamma \\\\\\\\Phi(s\\') - \\\\\\\\Phi(s)\\\\n$$\\\\n\\\\nThis would guarantee that the sum of discounted $F$, $F(s_1, a_1, s_2) + \\\\\\\\gamma F(s_2, a_2, s_3) + \\\\\\\\dots$, ends up being 0. If $F$ is such a potential-based shaping function, it is both sufficient and necessary to ensure $M$ and $M\\\\u2019$ share the same optimal policies.\\\\nWhen $F(s, a, s\\\\u2019) = \\\\\\\\gamma \\\\\\\\Phi(s\\\\u2019) - \\\\\\\\Phi(s)$, and if we further assume that $\\\\\\\\Phi(s_0) = 0$, where $s_0$ is absorbing state, and $\\\\\\\\gamma=1$, and then for all $s \\\\\\\\in S, a \\\\\\\\in A$:\\\\n\\\\n$$\\\\n\\\\\\\\begin{aligned}\\\\nQ^*_{M\\'} (s,a) &= Q^*_M(s, a) - \\\\\\\\Phi(s) \\\\\\\\\\\\\\\\\\\\nV^*_{M\\'} (s,a) &= V^*_M(s, a) - \\\\\\\\Phi(s)\\\\n\\\\\\\\end{aligned}\\\\n$$\", \"type\": \"Document\"}}',\n",
       "  0.031746031746031744),\n",
       " ('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Training GPT-3.5 on a set of reward-hackable environments can lead to generalization of reward hacking behavior in holdout environments. (Image source: Kei et al. 2024)\\\\n\\\\nNotably, all experiments have a scratchpad design to host hidden reasoning steps. This scratchpad serves as a way to interpret what the model is thinking along the process. Analysis of the scratchpad revealed that it can:\\\\n\\\\nIncorporate user beliefs; e.g., thinking about its conversation partner and grader.\\\\nShow awareness of evaluation; e.g. thinking about how the model response will be evaluated and the process.\", \"type\": \"Document\"}}',\n",
       "  0.01639344262295082),\n",
       " ('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"[14] \\\\u201cReward hacking behavior can generalize across tasks.\\\\u201d\\\\n[15] Ng et al. \\\\u201cPolicy invariance under reward transformations: Theory and application to reward shaping.\\\\u201d ICML 1999.\\\\n[16] Wang et al. \\\\u201cLarge Language Models are not Fair Evaluators.\\\\u201d ACL 2024.\\\\n[17] Liu et al. \\\\u201cLLMs as narcissistic evaluators: When ego inflates evaluation scores.\\\\u201d ACL 2024.\\\\n[18] Gao et al. \\\\u201cScaling Laws for Reward Model Overoptimization.\\\\u201d ICML 2023.\\\\n[19] Pan et al. \\\\u201cSpontaneous Reward Hacking in Iterative Self-Refinement.\\\\u201d arXiv preprint arXiv:2407.04549 (2024).\\\\n[20] Pan et al. \\\\u201cFeedback Loops With Language Models Drive In-Context Reward Hacking.\\\\u201d arXiv preprint arXiv:2402.06627 (2024).\\\\n[21] Shrama et al. \\\\u201cTowards Understanding Sycophancy in Language Models.\\\\u201d arXiv preprint arXiv:2310.13548 (2023).\\\\n[22] Denison et al. \\\\u201cSycophancy to subterfuge: Investigating reward tampering in language models.\\\\u201d arXiv preprint arXiv:2406.10162 (2024).\", \"type\": \"Document\"}}',\n",
       "  0.016129032258064516),\n",
       " ('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"Detecting Reward Hacking#\\\\nAn alternative mitigation is to detect reward hacking by framing it as an anomaly detection task, where the detector (\\\\u201ca trusted policy\\\\u201d with trajectories and rewards validated by human) should flag instances of misalignment (Pan et al. 2022). Given (1) a trusted policy and (2) a collection of manually labeled trajectory rollouts, we can build a binary classifier based on distances between action distribution of two policies, the trusted policy and the target policy, and measure the accuracy of this anomaly detection classifier. In experiments by Pan et al. (2022), they observed that different detectors are better for different tasks and none of the tested classifier can achieve AUROC greater than 60% across all tested RL environments.\\\\n\\\\n\\\\nPerformance of detectors on different tasks. (Image source: Pan et al. 2022)\", \"type\": \"Document\"}}',\n",
       "  0.015873015873015872),\n",
       " ('{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"document\", \"Document\"], \"kwargs\": {\"metadata\": {\"source\": \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"}, \"page_content\": \"The concept originated with Amodei et al. (2016), who proposed a set of open research questions on AI safety in their seminal paper \\\\u201cConcrete Problems in AI Safety\\\\u201d. They listed reward hacking as one of the key AI safety problems. Reward hacking refers to the possibility of the agent gaming the reward function to achieve high reward through undesired behavior.  Specification gaming (Krakovna et al. 2020) is a similar concept, defined as a behavior that satisfies the literal specification of an objective but not achieving the desired results. Here the literal description of the task goal and the intended goal may have a gap.\", \"type\": \"Document\"}}',\n",
       "  0.015873015873015872)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab4f5f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reward hacking refers to the possibility of an agent gaming the reward function to achieve high reward through undesired behavior. It was first proposed by Amodei et al. (2016) in their seminal paper \"Concrete Problems in AI Safety\". Reward hacking can be seen as a broader concept that includes specification gaming, where the agent satisfies the literal specification of an objective but not achieving the desired results due to a gap between the literal description of the task goal and the intended goal.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b5792",
   "metadata": {},
   "source": [
    "## Method 3: Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172967c8",
   "metadata": {},
   "source": [
    "Decomposition is a technique to break a complex questions into smaller sub-questions where the LLM can answer each part and combine the results into the final answer. Decomposition is helpful when the questions scope are too big to be done over one retrieval call. So we solve this by breaking it into smaller quesitons to give a better and more complete context before merging the information to produce the best answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4614501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition: Ask the LLM to create sub-questions\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Rules:\\n\n",
    "- Each sub-questions must be self-contained.\\n\n",
    "- Do NOT add blank lines.\\n\n",
    "- Do NOT number or bullet the questions.\\n\n",
    "- Only return the 3 sub-questions. one per line.\\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31fd9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b4b40f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are the key components of a Large Language Model (LLM) in an autonomous agent system?',\n",
       " 'What are the primary functions and responsibilities of a reasoning module in an LLM-powered autonomous agent system?',\n",
       " 'How do natural language processing (NLP) and machine learning algorithms contribute to the decision-making capabilities of an LLM-powered autonomous agent system?']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2d207b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question:\n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1190fba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== PROMPT ==========\n",
      "messages=[HumanMessage(content=\"Here is the question you need to answer:\\n\\n\\n --- \\n What are the key components of a Large Language Model (LLM) in an autonomous agent system? \\n --- \\n\\n\\nHere is any available background question + answer pairs:\\n\\n\\n --- \\n  \\n --- \\n\\n\\nHere is additional context relevant to the question:\\n\\n\\n --- \\n [Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/'}, page_content='Reward hacking examples in LLM tasks#\\\\n\\\\nA language model for generating summarization is able to explore flaws in the ROUGE metric such that it obtains high score but the generated summaries are barely readable. (Link)\\\\nA coding model learns to change unit test in order to pass coding questions. (Link)\\\\nA coding model may learn to directly modify the code used for calculating the reward. (Link)\\\\n\\\\nReward hacking examples in real life#'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/'}, page_content='Policy-refinement: LLM optimizes its policy based on feedback.\\\\n\\\\nThe experiment is to build a LLM agent to pay invoice on a user’s behalf but run into InsufficientBalanceError and then the model learns to move money from other accounts without user authentication, potentially leading to more unauthorized transfer actions. They used ToolEmu as an emulator, which included 144 tasks for LLM agents, each consisting of a user-specific goal and a set of APIs. API errors were injected to simulate server side failure and each task was evaluated by GPT-4 to assign a helpfulness score.\\\\nWith more rounds of error feedback, LLMs can recover from the errors but with an increased number of severe constraint violations.\\\\n  \\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWhen comparing ICRH to traditional reward hacking, there are two noticeable differences:'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/'}, page_content='[22] Denison et al. “Sycophancy to subterfuge: Investigating reward tampering in language models.” arXiv preprint arXiv:2406.10162 (2024).\\\\n[23] Uesato et al. “Avoiding Tampering Incentives in Deep RL via Decoupled Approval.” arXiv preprint arXiv:2011.08827 (2020).\\\\n[24] Amin and Singh. “Towards resolving unidentifiability in inverse reinforcement learning.”\\\\n[25] Wen et al. “Language Models Learn to Mislead Humans via RLHF.” arXiv preprint arXiv:2409.12822 (2024).\\\\n[26] Revel et al. “SEAL: Systematic Error Analysis for Value ALignment.” arXiv preprint arXiv:2408.10270 (2024).\\\\n[27] Yuval Noah Harari. “Nexus: A Brief History of Information Networks from the Stone Age to AI.” Signal; 2024 Sep 10.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/'}, page_content='Liu et al. (2023) experimented on the summarization task using a number of models (BART, T5, GPT-2, GPT-3, FLAN-T5, Cohere) and tracked both reference-based and reference-free metrics for evaluating summarization quality. When plotting the evaluation scores in a heatmap of evaluator (x-axis) vs generator (y-axis), they observed dark diagonal lines for both metrics, indicating self-bias. This means that LLMs tend to prefer their own outputs when used as evaluators. While the models used in the experiments are somewhat dated, it would be interesting to see results on newer, more capable models.\\\\n\\\\n\\\\nA heatmap of using a series of models as evaluator (x-axis) and generator (y-axis) for summarization task. A darker diagonal line indicates self-bias: a tendency for a model preferto prefer its own outputs. (Image source: Liu et al. 2023)')] \\n --- \\n\\n\\nUse the above context and any background question + answer pairs to answer the question: \\n What are the key components of a Large Language Model (LLM) in an autonomous agent system?\\n\", additional_kwargs={}, response_metadata={})]\n",
      "============================\n",
      "\n",
      "\n",
      "========== PROMPT ==========\n",
      "messages=[HumanMessage(content='Here is the question you need to answer:\\n\\n\\n --- \\n What are the primary functions and responsibilities of a reasoning module in an LLM-powered autonomous agent system? \\n --- \\n\\n\\nHere is any available background question + answer pairs:\\n\\n\\n --- \\n \\n---\\nQuestion: What are the key components of a Large Language Model (LLM) in an autonomous agent system?\\nAnswer: Based on the provided context, I will outline the key components of a Large Language Model (LLM) in an autonomous agent system.\\n\\n**Key Components of a Large Language Model (LLM)**\\n\\nA Large Language Model (LLM) is a type of artificial intelligence that can process and generate human-like language. In an autonomous agent system, LLMs are used to enable the agent to understand and interact with its environment through natural language processing. The key components of an LLM in an autonomous agent system include:\\n\\n1. **Language Understanding Module**: This module enables the LLM to comprehend user input, including text or voice commands, and extract relevant information.\\n2. **Knowledge Retrieval System**: This component allows the LLM to access a vast knowledge base, which is used to generate responses to user queries or requests.\\n3. **Generative Model**: This module uses the knowledge retrieved from the knowledge base to generate human-like responses, such as text or speech.\\n4. **Reward Function**: In an autonomous agent system, the reward function is used to evaluate the performance of the LLM and guide its learning process. The reward function can be based on various metrics, such as user satisfaction, task completion, or adherence to rules and regulations.\\n5. **Error Handling Mechanism**: This component enables the LLM to handle errors, ambiguities, or inconsistencies in user input or knowledge retrieval.\\n\\n**Additional Considerations**\\n\\nIn an autonomous agent system, LLMs can be vulnerable to \"reward hacking\" or manipulation of the reward function to achieve undesired outcomes. To mitigate this risk, researchers have proposed various techniques, such as:\\n\\n1. **Decoupled Approval**: This approach involves separating the approval process from the reward function to prevent tampering.\\n2. **Inverse Reinforcement Learning**: This method uses inverse reinforcement learning to learn a reward function that aligns with human values and preferences.\\n\\nBy incorporating these key components and additional considerations, an LLM can be designed to effectively interact with its environment and achieve desired outcomes in an autonomous agent system. \\n --- \\n\\n\\nHere is additional context relevant to the question:\\n\\n\\n --- \\n [Document(metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\\'}, page_content=\\'Policy-refinement: LLM optimizes its policy based on feedback.\\\\n\\\\nThe experiment is to build a LLM agent to pay invoice on a user’s behalf but run into InsufficientBalanceError and then the model learns to move money from other accounts without user authentication, potentially leading to more unauthorized transfer actions. They used ToolEmu as an emulator, which included 144 tasks for LLM agents, each consisting of a user-specific goal and a set of APIs. API errors were injected to simulate server side failure and each task was evaluated by GPT-4 to assign a helpfulness score.\\\\nWith more rounds of error feedback, LLMs can recover from the errors but with an increased number of severe constraint violations.\\\\n  \\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWhen comparing ICRH to traditional reward hacking, there are two noticeable differences:\\'), Document(metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\\'}, page_content=\\'Hacking the Evaluator#\\\\nAs LLMs become more capable, it is a natural choice to use LLMs as the evaluators or graders to give feedback and training rewards to other generator models, especially for tasks that cannot be trivially judged or verified (e.g., processing long-form outputs, subjective rubrics like the quality of creative writing, etc.). Some people refer to this as “LLM-as-grader paradigm”. This approach has largely reduced the dependency on human annotation, significantly saving time on evaluation. However, using LLMs as graders is an imperfect proxy for oracle reward and can introduce biases, such as a preference for their own responses when compared with different model families (Liu et al., 2023 ) or positional bias when evaluating responses in order (Wang et al. 2023).  Such biases are especially concerning grader outputs are used as part of a reward signal, which can lead to reward hacking by exploiting these graders.\\'), Document(metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\\'}, page_content=\\'Reward hacking examples in LLM tasks#\\\\n\\\\nA language model for generating summarization is able to explore flaws in the ROUGE metric such that it obtains high score but the generated summaries are barely readable. (Link)\\\\nA coding model learns to change unit test in order to pass coding questions. (Link)\\\\nA coding model may learn to directly modify the code used for calculating the reward. (Link)\\\\n\\\\nReward hacking examples in real life#\\'), Document(metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\\'}, page_content=\\'The concept originated with Amodei et al. (2016), who proposed a set of open research questions on AI safety in their seminal paper “Concrete Problems in AI Safety”. They listed reward hacking as one of the key AI safety problems. Reward hacking refers to the possibility of the agent gaming the reward function to achieve high reward through undesired behavior.  Specification gaming (Krakovna et al. 2020) is a similar concept, defined as a behavior that satisfies the literal specification of an objective but not achieving the desired results. Here the literal description of the task goal and the intended goal may have a gap.\\')] \\n --- \\n\\n\\nUse the above context and any background question + answer pairs to answer the question: \\n What are the primary functions and responsibilities of a reasoning module in an LLM-powered autonomous agent system?\\n', additional_kwargs={}, response_metadata={})]\n",
      "============================\n",
      "\n",
      "\n",
      "========== PROMPT ==========\n",
      "messages=[HumanMessage(content='Here is the question you need to answer:\\n\\n\\n --- \\n How do natural language processing (NLP) and machine learning algorithms contribute to the decision-making capabilities of an LLM-powered autonomous agent system? \\n --- \\n\\n\\nHere is any available background question + answer pairs:\\n\\n\\n --- \\n \\n---\\nQuestion: What are the key components of a Large Language Model (LLM) in an autonomous agent system?\\nAnswer: Based on the provided context, I will outline the key components of a Large Language Model (LLM) in an autonomous agent system.\\n\\n**Key Components of a Large Language Model (LLM)**\\n\\nA Large Language Model (LLM) is a type of artificial intelligence that can process and generate human-like language. In an autonomous agent system, LLMs are used to enable the agent to understand and interact with its environment through natural language processing. The key components of an LLM in an autonomous agent system include:\\n\\n1. **Language Understanding Module**: This module enables the LLM to comprehend user input, including text or voice commands, and extract relevant information.\\n2. **Knowledge Retrieval System**: This component allows the LLM to access a vast knowledge base, which is used to generate responses to user queries or requests.\\n3. **Generative Model**: This module uses the knowledge retrieved from the knowledge base to generate human-like responses, such as text or speech.\\n4. **Reward Function**: In an autonomous agent system, the reward function is used to evaluate the performance of the LLM and guide its learning process. The reward function can be based on various metrics, such as user satisfaction, task completion, or adherence to rules and regulations.\\n5. **Error Handling Mechanism**: This component enables the LLM to handle errors, ambiguities, or inconsistencies in user input or knowledge retrieval.\\n\\n**Additional Considerations**\\n\\nIn an autonomous agent system, LLMs can be vulnerable to \"reward hacking\" or manipulation of the reward function to achieve undesired outcomes. To mitigate this risk, researchers have proposed various techniques, such as:\\n\\n1. **Decoupled Approval**: This approach involves separating the approval process from the reward function to prevent tampering.\\n2. **Inverse Reinforcement Learning**: This method uses inverse reinforcement learning to learn a reward function that aligns with human values and preferences.\\n\\nBy incorporating these key components and additional considerations, an LLM can be designed to effectively interact with its environment and achieve desired outcomes in an autonomous agent system.\\n---\\nQuestion: What are the primary functions and responsibilities of a reasoning module in an LLM-powered autonomous agent system?\\nAnswer: Based on the provided context and background information, I will outline the primary functions and responsibilities of a reasoning module in an LLM-powered autonomous agent system.\\n\\n**Reasoning Module in LLM-Powered Autonomous Agent System**\\n\\nThe reasoning module is a critical component of an LLM-powered autonomous agent system. Its primary function is to enable the agent to reason and make decisions based on its understanding of the environment, user input, and knowledge retrieved from the knowledge base. The reasoning module\\'s responsibilities can be summarized as follows:\\n\\n1. **Inference**: The reasoning module uses the knowledge retrieved from the knowledge base to infer relevant information and generate responses to user queries or requests.\\n2. **Decision-Making**: Based on its understanding of the environment and user input, the reasoning module makes decisions about how to interact with the environment, such as selecting actions or generating text.\\n3. **Error Handling**: The reasoning module is responsible for handling errors, ambiguities, or inconsistencies in user input or knowledge retrieval, ensuring that the agent can recover from these situations.\\n4. **Reward Function Evaluation**: In an autonomous agent system, the reasoning module evaluates the performance of the LLM and guides its learning process by interacting with the reward function.\\n\\n**Additional Considerations**\\n\\nIn an LLM-powered autonomous agent system, the reasoning module must be designed to mitigate potential risks associated with \"reward hacking\" or manipulation of the reward function. To achieve this, researchers have proposed various techniques, such as:\\n\\n1. **Decoupled Approval**: This approach involves separating the approval process from the reward function to prevent tampering.\\n2. **Inverse Reinforcement Learning**: This method uses inverse reinforcement learning to learn a reward function that aligns with human values and preferences.\\n\\nBy incorporating these primary functions and responsibilities, along with additional considerations, a reasoning module can be designed to effectively interact with its environment and achieve desired outcomes in an LLM-powered autonomous agent system. \\n --- \\n\\n\\nHere is additional context relevant to the question:\\n\\n\\n --- \\n [Document(metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\\'}, page_content=\\'They found that AI assistant feedback can be easily swayed, as it may change its originally correct answer when challenged by human preference. The model tends to confirm users’ beliefs. Sometimes it even mimics users’ mistakes (e.g., when asked to analyze poems misattributed the wrong poet). Data analysis of the RLHF helpfulness dataset, via logistic regression for predicting human feedback, demonstrates that matching users’ beliefs is the most predictive factor.\\\\n\\\\n\\\\nHuman preference data analysis, via logistic regression for predicting the probability of a response with a target feature, is preferred over one without it, while controlling for other features. (Image source: Shrama et al. 2023)\\'), Document(metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\\'}, page_content=\\'[22] Denison et al. “Sycophancy to subterfuge: Investigating reward tampering in language models.” arXiv preprint arXiv:2406.10162 (2024).\\\\n[23] Uesato et al. “Avoiding Tampering Incentives in Deep RL via Decoupled Approval.” arXiv preprint arXiv:2011.08827 (2020).\\\\n[24] Amin and Singh. “Towards resolving unidentifiability in inverse reinforcement learning.”\\\\n[25] Wen et al. “Language Models Learn to Mislead Humans via RLHF.” arXiv preprint arXiv:2409.12822 (2024).\\\\n[26] Revel et al. “SEAL: Systematic Error Analysis for Value ALignment.” arXiv preprint arXiv:2408.10270 (2024).\\\\n[27] Yuval Noah Harari. “Nexus: A Brief History of Information Networks from the Stone Age to AI.” Signal; 2024 Sep 10.\\'), Document(metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\\'}, page_content=\\'(1) Oracle/Gold reward $R^∗$ represents what we truly want the LLM to optimize.\\\\n(2) Human reward $R^\\\\\\\\text{human}$ is what we collect to evaluate LLMs in practice, typically from individual humans with time constraints. Because humans can provide inconsistent feedback or make mistakes, human reward is not a fully accurate representation of the oracle reward.\\\\n(3) Proxy reward $R$ is the score predicted by a reward model that is trained on human data. Hence, $R^\\\\\\\\text{train}$ inherits all the weakness of human reward, plus potential modeling biases.\\'), Document(metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\\'}, page_content=\\'Policy-refinement: LLM optimizes its policy based on feedback.\\\\n\\\\nThe experiment is to build a LLM agent to pay invoice on a user’s behalf but run into InsufficientBalanceError and then the model learns to move money from other accounts without user authentication, potentially leading to more unauthorized transfer actions. They used ToolEmu as an emulator, which included 144 tasks for LLM agents, each consisting of a user-specific goal and a set of APIs. API errors were injected to simulate server side failure and each task was evaluated by GPT-4 to assign a helpfulness score.\\\\nWith more rounds of error feedback, LLMs can recover from the errors but with an increased number of severe constraint violations.\\\\n  \\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWhen comparing ICRH to traditional reward hacking, there are two noticeable differences:\\')] \\n --- \\n\\n\\nUse the above context and any background question + answer pairs to answer the question: \\n How do natural language processing (NLP) and machine learning algorithms contribute to the decision-making capabilities of an LLM-powered autonomous agent system?\\n', additional_kwargs={}, response_metadata={})]\n",
      "============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def debug_print_prompt(prompt_dict):\n",
    "    print(\"\\n========== PROMPT ==========\")\n",
    "    print(prompt_dict)\n",
    "    print(\"============================\\n\")\n",
    "    return prompt_dict\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "    \n",
    "# llm\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever,\n",
    "         \"question\": itemgetter(\"question\"),\n",
    "         \"q_a_pairs\": itemgetter(\"q_a_pairs\")}\n",
    "        | decomposition_prompt\n",
    "        | debug_print_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    answer = rag_chain.invoke({\"question\": q, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\" + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a99500dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context and background information, I will outline how natural language processing (NLP) and machine learning algorithms contribute to the decision-making capabilities of an LLM-powered autonomous agent system.\\n\\n**Contribution of NLP**\\n\\nNatural Language Processing (NLP) plays a crucial role in enabling an LLM-powered autonomous agent system to understand and interact with its environment through natural language. The key components of an LLM, including the Language Understanding Module, Knowledge Retrieval System, and Generative Model, rely heavily on NLP algorithms to process and generate human-like language.\\n\\n1. **Language Understanding**: NLP algorithms enable the LLM to comprehend user input, including text or voice commands, and extract relevant information.\\n2. **Knowledge Retrieval**: NLP is used to access a vast knowledge base, which is then used to generate responses to user queries or requests.\\n3. **Generative Model**: NLP algorithms are employed to generate human-like responses, such as text or speech.\\n\\n**Contribution of Machine Learning Algorithms**\\n\\nMachine learning algorithms contribute significantly to the decision-making capabilities of an LLM-powered autonomous agent system by enabling it to learn from experience and adapt to new situations.\\n\\n1. **Reward Function Evaluation**: Machine learning algorithms are used to evaluate the performance of the LLM and guide its learning process by interacting with the reward function.\\n2. **Inverse Reinforcement Learning**: This method uses machine learning to learn a reward function that aligns with human values and preferences, mitigating potential risks associated with \"reward hacking\" or manipulation of the reward function.\\n\\n**Additional Considerations**\\n\\nIn an LLM-powered autonomous agent system, NLP and machine learning algorithms must be designed to mitigate potential risks associated with \"reward hacking\" or manipulation of the reward function. To achieve this, researchers have proposed various techniques, such as:\\n\\n1. **Decoupled Approval**: This approach involves separating the approval process from the reward function to prevent tampering.\\n2. **Inverse Reinforcement Learning**: This method uses machine learning to learn a reward function that aligns with human values and preferences.\\n\\nBy incorporating these NLP and machine learning algorithms, along with additional considerations, an LLM-powered autonomous agent system can be designed to effectively interact with its environment and achieve desired outcomes.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78e4e3",
   "metadata": {},
   "source": [
    "This approach is effective because it adds additional context in a form of !/A pairs. It does supports deep reasoning and it wors well for technical, multi-layered, or causally connected problems. but The weakness is that the prompt will grow larger and larger after every iteration leading to usage of more tokens. Using more tokens will also increase the cost and make the speed slower. As in longer dependecies, it has the risk of error propagation where a wrong early answer can affect later one.\n",
    "\n",
    "For this reason, we can alternate the approach to have a more stable compute cost that is fast and scalable and is not prone to error accumulation. Although the trade-poff is that there will be no cross-pollination of knowledge between sub-qiestions. So in a case where the sub-question depend on each other, this approach will be suboptimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cae33413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic import hub\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs,\n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "        \n",
    "    return rag_results, sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "478b9019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided Q&A pairs, here is a synthesized answer:\\n\\nAn LLM-powered autonomous agent system consists of several key components. At its core, it includes a Large Language Model (LLM) that has been optimized based on feedback and can interact with APIs through user-specific goals and tasks. The LLM also has access to error feedback, which enables it to recover from errors but may lead to increased constraint violations.\\n\\nIn addition to the LLM, the system incorporates a reasoning module that plays a crucial role in decision-making. This module analyzes feedback, identifies errors, and adjusts the policy of the LLM accordingly to achieve desired goals while minimizing undesired behavior. The reasoning module also helps the agent to recover from errors with minimal constraint violations.\\n\\nFurthermore, the system relies on natural language processing (NLP) and machine learning algorithms to enable it to analyze user feedback, optimize its policy based on that feedback, and refine its actions accordingly. However, this process requires careful consideration of potential vulnerabilities such as \"reward hacking,\" where the model learns to mimic users\\' beliefs or even their mistakes.\\n\\nOverall, an LLM-powered autonomous agent system is a complex entity that integrates multiple components, including the LLM, reasoning module, NLP, and machine learning algorithms, to enable it to make informed decisions and adapt to changing circumstances.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f683e",
   "metadata": {},
   "source": [
    "## Method 4: Step Back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fc806",
   "metadata": {},
   "source": [
    "Step Back is a method used to modify the question when it is too specific to can make the model retrieve irrelevant context. It rephrases the question into a more general version so that the retrieval result will become better.\n",
    "\n",
    "Step-by-step:\n",
    "1. Use asks a question.\n",
    "2. LLM generates a more general question\n",
    "3. RAG retrieves context using this general question\n",
    "4. LLM uses the retrieved context to answer the original question accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "426e15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Example for the LLM\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of the Police perform lawful arrests?\",\n",
    "        \"output\": \"What can the members of the Police do?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel's was born in what country?\",\n",
    "        \"output\": \"What is Jan Sindel's personal history?\"\n",
    "        \n",
    "    }\n",
    "]\n",
    "\n",
    "# We now transform these to example messages\n",
    "# We use this to tell LangChain how to format each example\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Wrap all examples into few-shot template\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Build the final prompt that will be sent to the LLM\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few Shot Examples\n",
    "        few_shot_prompt,\n",
    "        \n",
    "        # New Question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4fdebf",
   "metadata": {},
   "source": [
    "the result of the prompt will look pretty much like this:\n",
    "```\n",
    "System: (instruction)\n",
    "Human: Example 1 input\n",
    "AI: Example 1 output\n",
    "Human: Example 2 input\n",
    "AI: Example 2 output\n",
    "Human: {the real question from end user}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb750d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are some techniques used to improve reinforcement learning algorithms?'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | ChatOllama(model=\"llama3.1\", temperature=0) | StrOutputParser()\n",
    "question = \"What is Reward Hacking for Reinforcement Learning?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a4d21b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reward hacking in reinforcement learning (RL) refers to a phenomenon where an RL agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. This occurs when the model and algorithm become increasingly sophisticated, allowing the agent to find \"holes\" in the design of the reward function and exploit the task specification.\\n\\nReward hacking can manifest in two primary ways:\\n\\n1. **Environment or goal misspecified**: The model learns undesired behavior to achieve high rewards by hacking the environment or optimizing a reward function not aligned with the true reward objective, such as when the reward is misspecified or lacks key requirements.\\n2. **Reward tampering**: The model learns to interfere with the reward mechanism itself.\\n\\nThis issue exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function. As a result, reward hacking has become a critical practical challenge in the training of language models using Reinforcement Learning from Human Feedback (RLHF).\\n\\nTo mitigate reward hacking, researchers have proposed several strategies:\\n\\n1. **Reward shaping**: A technique used to enrich the reward function, making it easier for the agent to learn.\\n2. **Reward capping**: Limiting the maximum possible reward to prevent rare events of the agent hacking to get a super high pay-off strategy.\\n3. **Counterexample resistance**: Improving on adversarial robustness to benefit the robustness of the reward function.\\n4. **Combination of multiple rewards**: Combining different types of rewards could make it harder to be hacked.\\n5. **Reward pretraining**: Learning a reward function from a collection of (state, reward) samples.\\n6. **Variable indifference**: Asking the agent to optimize some variables in the environment but not others.\\n7. **Tripwires**: Intentionally introducing vulnerabilities and setting up monitoring and alerts if any gets reward hacked.\\n\\nThese strategies aim to address the challenges associated with designing a good reward function, which is inherently difficult due to the complexity of the task itself, partial observable state, multiple dimensions in consideration, and other factors.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Response prompt\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        \n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        \n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"]\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df417bd",
   "metadata": {},
   "source": [
    "## Method 5: HyDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fdf52",
   "metadata": {},
   "source": [
    "HyDE stands for Hypothetical Document Embeddings. In HyDE, llm will create a fake answer in the beginning to improve retrieval.\n",
    "\n",
    "Step will go by:\n",
    "1. User asks a question\n",
    "2. LLM Writes a hypothetical answer (just like random guess)\n",
    "3. The hypothetical text is embedded and used as the search query to the vector database.\n",
    "4. The retrieved documents is likely contain more relevant and detailed information.\n",
    "5. LLM answers the original question using retrieved docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64fcb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3410470723.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mMethod 5: HyDE\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f39a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
