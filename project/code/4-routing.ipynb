{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134d3b0b",
   "metadata": {},
   "source": [
    "# RAG from Scratch: Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a1cfea",
   "metadata": {},
   "source": [
    "In using RAG, the model often receives the wrong context, either it's too broad, too narrow ot not relevant.\n",
    "\n",
    "These are some things that can make this happen:\n",
    "1. Not every question needs retrieval, yet the model pipeline will retrive the data in every cases.\n",
    "2. Knowledge sources can come in different formats (documents, SQL, databases, code, APIs, etc.)\n",
    "3. One single retriever cannot fit every task.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "For this reason, we can implement extra step after doing the query transformation step, which is the **Routing**. Routing is a step where we choose to most appropriate source for a query before retrieval, because the right source might be stored in different database or even different database types (relational db, graph db, and vector stores).\n",
    "\n",
    "Without routing, we would send every question to the same retriever every single time. By implementing routing system. the routing can then decide the following issue:\n",
    "1. Whether the particular question need retrieval or not.\n",
    "2. If yes, which retriever or knowledge database should be used?\n",
    "3. If no, should the LLM answer directly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2f439",
   "metadata": {},
   "source": [
    "## Method 1: Logical Routing\n",
    "\n",
    "We give the LLM knowledge of the various data sources that we have at our disposal, and we let the LLM to use its logic to reason which database to be used and apply the question to.\n",
    "\n",
    "Logical Routing is based on rules or categories. It will classify the question and decide the action. When we ask something like \"Explain quantum computing\" it will route to scientific pages like wikipedia. But for question like \"Where is the nearest ATM\", it will route it to a real-time API retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0486aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q langchain_community tiktoken langchain-ollama langchainhub chromadb langchain youtube-transcript-api pytube pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41459f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from access import Access\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = Access.LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c4a8346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasrouce.\"\"\"\n",
    "    \n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be the most relevant for answering their question\",\n",
    "    )\n",
    "    \n",
    "# LLM with Function call\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6802795",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8533b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='python_docs')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad61985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_docs'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd282cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the chain\n",
    "# ---- your answer chains (very simple example) ----\n",
    "answer_llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "python_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert in Python. Answer clearly.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "js_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert in JavaScript. Answer clearly.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "golang_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert in Golang. Answer clearly.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "python_chain = python_prompt | answer_llm\n",
    "js_chain = js_prompt | answer_llm\n",
    "golang_chain = golang_prompt | answer_llm\n",
    "\n",
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        # Chain for python docs\n",
    "        return python_chain\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        # Chain for js docs\n",
    "        return js_chain\n",
    "    else:\n",
    "        ### Logic here \n",
    "        # Chain for golang docs\n",
    "        return golang_chain\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route) | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c41cab8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It looks like you're setting a variable named `datasource` to the string `'python_docs'`.\\n\\nIn Python, this is simply assigning a value to a variable. The variable `datasource` now holds the string `'python_docs'`. \\n\\nHere's an example of how you might use it:\\n```python\\ndatasource = 'python_docs'\\nprint(datasource)  # Outputs: python_docs\\n```\\nIs there something specific you'd like to do with this variable, or would you like more information on working with variables in Python?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95999882",
   "metadata": {},
   "source": [
    "## Method 2: Semantic Routing\n",
    "\n",
    "Sematic routing is a method that is based on embeddings similarity. In this method, we don't define rules. The step will be as follows:\n",
    "1. We convert the question into a vector (embedding)\n",
    "2. Compares it to vector \"profiles\" of each retriever\n",
    "3. Picks the retriever with the closest semantic match\n",
    "\n",
    "With this approach, we can have different knowledge base each for medical, finance, and even more. This method is more flecible and works even when the topic is vague or overlapping. It is good for large or unstructured knowledge spaces. The disadvantage is that we need embeddings and vector search to implement this and it is also harder to debug because the decision is not rule based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cd82cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/43/5d/779320063e88af9c4a7c2cf463ff11c21ac9c8bd730c4a294b0000b666c9/scikit_learn-1.7.2-cp312-cp312-macosx_12_0_arm64.whl.metadata\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from scikit-learn) (2.3.5)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.8.0 from https://files.pythonhosted.org/packages/96/5e/36bf3f0ac298187d1ceadde9051177d6a4fe4d507e8f59067dc9dd39e650/scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata\n",
      "  Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.2.0 from https://files.pythonhosted.org/packages/1e/e8/685f47e0d754320684db4425a0967f7d3fa70126bffd76110b7009a0090f/joblib-1.5.2-py3-none-any.whl.metadata\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=3.1.0 from https://files.pythonhosted.org/packages/32/d5/f9a850d79b0851d1d4ef6456097579a9005b31fea68726a4ae5f2d82ddd9/threadpoolctl-3.6.0-py3-none-any.whl.metadata\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-macosx_12_0_arm64.whl (8.6 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540976e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MATH\n",
      "A black hole! A fascinating topic in physics and mathematics. To understand what a black hole is, let's break it down into its fundamental components:\n",
      "\n",
      "**Component 1: Mass**\n",
      "A black hole is formed when a massive object, such as a star, collapses under its own gravity. The mass of the object determines the strength of its gravitational pull.\n",
      "\n",
      "**Mathematical Representation:** Let's use the concept of mass-energy equivalence from Einstein's theory of relativity (E=mc²). We can represent the mass of an object using the symbol \"m\".\n",
      "\n",
      "**Component 2: Gravity**\n",
      "The gravity of a massive object warps the fabric of spacetime around it, creating a gravitational field. The strength of this field depends on the mass and radius of the object.\n",
      "\n",
      "**Mathematical Representation:** We can use the concept of curvature from general relativity to describe the gravitational field. Let's represent the curvature using the Riemann tensor (Rμν).\n",
      "\n",
      "**Component 3: Event Horizon**\n",
      "As a massive object collapses, its gravity becomes so strong that not even light can escape once it crosses a certain boundary called the event horizon.\n",
      "\n",
      "**Mathematical Representation:** The event horizon is defined by the Schwarzschild radius (r_s), which depends on the mass of the black hole. We can represent this using the equation:\n",
      "\n",
      "r_s = 2GM/c²\n",
      "\n",
      "where G is the gravitational constant, M is the mass of the black hole, and c is the speed of light.\n",
      "\n",
      "**Putting it all together:**\n",
      "Now that we've broken down the components, let's combine them to understand what a black hole is. A black hole is a region in spacetime where the gravity is so strong that not even light can escape once it crosses the event horizon. The strength of this gravity depends on the mass and radius of the object.\n",
      "\n",
      "**Mathematical Representation:** We can represent the properties of a black hole using a combination of the above components:\n",
      "\n",
      "* Mass (m)\n",
      "* Gravity (Rμν)\n",
      "* Event Horizon (r_s)\n",
      "\n",
      "In summary, a black hole is a region in spacetime with such strong gravity that not even light can escape once it crosses the event horizon. The strength of this gravity depends on the mass and radius of the object.\n",
      "\n",
      "How's that? Did I break down the problem into manageable components and put them together to answer the question?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# Two prompts\n",
    "physic_template = \"\"\"You are a very smart physic professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# Embed prompts\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.1\")\n",
    "prompt_templates = [physic_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# Route question to prompt\n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    # Chosen prompt\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "    \n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5ade7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3363744c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
