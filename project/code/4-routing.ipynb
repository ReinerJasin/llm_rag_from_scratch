{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "134d3b0b",
   "metadata": {},
   "source": [
    "# RAG from Scratch: Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a1cfea",
   "metadata": {},
   "source": [
    "In using RAG, the model often receives the wrong context, either it's too broad, too narrow ot not relevant.\n",
    "\n",
    "These are some things that can make this happen:\n",
    "1. Not every question needs retrieval, yet the model pipeline will retrive the data in every cases.\n",
    "2. Knowledge sources can come in different formats (documents, SQL, databases, code, APIs, etc.)\n",
    "3. One single retriever cannot fit every task.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "For this reason, we can implement extra step after doing the query transformation step, which is the **Routing**. Routing is a step where we choose to most appropriate source for a query before retrieval, because the right source might be stored in different database or even different database types (relational db, graph db, and vector stores).\n",
    "\n",
    "Without routing, we would send every question to the same retriever every single time. By implementing routing system. the routing can then decide the following issue:\n",
    "1. Whether the particular question need retrieval or not.\n",
    "2. If yes, which retriever or knowledge database should be used?\n",
    "3. If no, should the LLM answer directly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2f439",
   "metadata": {},
   "source": [
    "## Method 1: Logical Routing\n",
    "\n",
    "We give the LLM knowledge of the various data sources that we have at our disposal, and we let the LLM to use its logic to reason which database to be used and apply the question to.\n",
    "\n",
    "Logical Routing is based on rules or categories. It will classify the question and decide the action. When we ask something like \"Explain quantum computing\" it will route to scientific pages like wikipedia. But for question like \"Where is the nearest ATM\", it will route it to a real-time API retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0486aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -q langchain_community tiktoken langchain-ollama langchainhub chromadb langchain youtube-transcript-api pytube pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41459f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from access import Access\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = Access.LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c4a8346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasrouce.\"\"\"\n",
    "    \n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be the most relevant for answering their question\",\n",
    "    )\n",
    "    \n",
    "# LLM with Function call\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6802795",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8533b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='python_docs')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad61985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_docs'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd282cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the chain\n",
    "# ---- your answer chains (very simple example) ----\n",
    "answer_llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "python_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert in Python. Answer clearly.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "js_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert in JavaScript. Answer clearly.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "golang_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert in Golang. Answer clearly.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "python_chain = python_prompt | answer_llm\n",
    "js_chain = js_prompt | answer_llm\n",
    "golang_chain = golang_prompt | answer_llm\n",
    "\n",
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        # Chain for python docs\n",
    "        return python_chain\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        # Chain for js docs\n",
    "        return js_chain\n",
    "    else:\n",
    "        ### Logic here \n",
    "        # Chain for golang docs\n",
    "        return golang_chain\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route) | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c41cab8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It looks like you're setting a variable named `datasource` to the string `'python_docs'`.\\n\\nIn Python, this is simply assigning a value to a variable. The variable `datasource` now holds the string `'python_docs'`. \\n\\nHere's an example of how you might use it:\\n```python\\ndatasource = 'python_docs'\\nprint(datasource)  # Outputs: python_docs\\n```\\nIs there something specific you'd like to do with this variable, or would you like more information on working with variables in Python?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95999882",
   "metadata": {},
   "source": [
    "## Method 2: Semantic Routing\n",
    "\n",
    "Sematic routing is a method that is based on embeddings similarity. In this method, we don't define rules. The step will be as follows:\n",
    "1. We convert the question into a vector (embedding)\n",
    "2. Compares it to vector \"profiles\" of each retriever\n",
    "3. Picks the retriever with the closest semantic match\n",
    "\n",
    "With this approach, we can have different knowledge base each for medical, finance, and even more. This method is more flecible and works even when the topic is vague or overlapping. It is good for large or unstructured knowledge spaces. The disadvantage is that we need embeddings and vector search to implement this and it is also harder to debug because the decision is not rule based."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363744c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
