{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ec3683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: langchain[google-genai] in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langchain[google-genai]) (1.1.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langchain[google-genai]) (1.0.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langchain[google-genai]) (2.12.4)\n",
      "Requirement already satisfied: langchain-google-genai in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langchain[google-genai]) (3.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain[google-genai]) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain[google-genai]) (0.4.48)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain[google-genai]) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain[google-genai]) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain[google-genai]) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.1.0->langchain[google-genai]) (4.15.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (0.2.10)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (3.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain[google-genai]) (0.4.2)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langchain-google-genai->langchain[google-genai]) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage<1.0.0,>=0.9.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langchain-google-genai->langchain[google-genai]) (0.9.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (2.28.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (2.43.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (1.76.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (6.33.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain[google-genai]) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain[google-genai]) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain[google-genai]) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain[google-genai]) (0.25.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (1.72.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (1.76.0)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (4.9.1)\n",
      "Requirement already satisfied: anyio in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain[google-genai]) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain[google-genai]) (2.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.9.0->langchain-google-genai->langchain[google-genai]) (0.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain[google-genai]) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain_core langchain_ollama\n",
    "!pip install -U \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6fb1964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from access import Access\n",
    "\n",
    "# os.environ['ANTHROPIC_API_KEY'] = Access.LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.getenv(\"ANTHROPIC_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7250ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 667b0c1932bc: 100% ▕██████████████████▏ 4.9 GB                         \u001b[K\n",
      "pulling 948af2743fc7: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling 0ba8f0e314b4: 100% ▕██████████████████▏  12 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 455f34728c9b: 100% ▕██████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20179065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "chat_model = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0.7,\n",
    "    # max_tokens=1000\n",
    ")\n",
    "\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# chat_model = ChatAnthropic(\n",
    "#   model=\"claude-3-sonnet-20240229\",\n",
    "#   temperature=0,\n",
    "#   api_key=\"YOUR_ANTHROPIC_API_KEY\"\n",
    "# )\n",
    "\n",
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# model = init_chat_model(\n",
    "#     \"claude-sonnet-4-5-20250929\",\n",
    "#     # Kwargs passed to the model:\n",
    "#     temperature=0.7,\n",
    "#     timeout=30,\n",
    "#     max_tokens=1000,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "763b3c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the penguin take his credit card to the Antarctic?\\n\\nBecause he wanted to freeze his assets.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-27T13:28:06.375898Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13496573541, 'load_duration': 11580159916, 'prompt_eval_count': 18, 'prompt_eval_duration': 635004125, 'eval_count': 22, 'eval_duration': 1144876417, 'logprobs': None, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--312e0c92-f841-4815-b1ae-e0e22ba6ba5f-0', usage_metadata={'input_tokens': 18, 'output_tokens': 22, 'total_tokens': 40})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"Tell me a joke about penguin!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e48779",
   "metadata": {},
   "source": [
    "The output shuold be in this format:\n",
    "```\n",
    "AIMessage(content=\"Here's a bear joke for you:\\\\n\\\\nWhy did the bear dissolve in water?\\\\nBecause it was a polar bear!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0819c3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the bear go to the doctor?\\n\\nBecause it had a grizzly cough!', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-27T13:28:11.976963Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1700390292, 'load_duration': 102051375, 'prompt_eval_count': 17, 'prompt_eval_duration': 508642333, 'eval_count': 18, 'eval_duration': 973108417, 'logprobs': None, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--a753a00e-8397-46d6-92de-84fc4f4d2852-0', usage_metadata={'input_tokens': 17, 'output_tokens': 18, 'total_tokens': 35})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way of doing this\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_model.invoke([\n",
    "    HumanMessage(\"Tell me a joke about bears!\")\n",
    "])\n",
    "\n",
    "# Just the same but this one doesn't take simple string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd48b2",
   "metadata": {},
   "source": [
    "Result:\n",
    "```\n",
    "AIMessage(content=\"Here's a bear joke for you:\\\\n\\\\nWhy did the bear bring a briefcase to work?\\\\nHe was a business bear!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de748f",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "We can also parameterize the input using prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a74abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class comedian.\"),\n",
    "    (\"human\", \"Tell me a joke about {topic}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1d392d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a world class comedian.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about beets', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke_prompt.invoke({\"topic\": \"beets\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b3485",
   "metadata": {},
   "source": [
    "Result:\n",
    "```\n",
    "ChatPromptValue(messages=[\n",
    "    SystemMessage(content='You are a world class comedian.'),\n",
    "    HumanMessage(content='Tell me a joke about beets')\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe800a",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "Both Prompt Template and Chat Model implement the `.invoke()` method because they are both instances of Runnables.\n",
    "\n",
    "We can compose runnables into \"chains\" using the pipe(`|`) operator where we invoke the next step with the output of the previous one. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9abe18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = joke_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e46a4687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here\\'s one:\\n\\n\"Why did the beet go to therapy? Because it was feeling a little \\'root\\'-less! (ba-dum-tss)\"', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-27T13:28:20.33072Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2381205125, 'load_duration': 99168208, 'prompt_eval_count': 29, 'prompt_eval_duration': 372763209, 'eval_count': 31, 'eval_duration': 1708107418, 'logprobs': None, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--48d7d7c6-24af-491c-b99c-d99680c19e86-0', usage_metadata={'input_tokens': 29, 'output_tokens': 31, 'total_tokens': 60})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"beets\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee0d668",
   "metadata": {},
   "source": [
    "Result should be like:\n",
    "```\n",
    "AIMessage(content=\"Here's a beet joke for you:\\\\n\\\\nWhy did the beet blush? Because it saw the salad dressing!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f28bc",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "\n",
    "Output Parser help us present the result as raw string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2b57e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "str_chain = chain | StrOutputParser()\n",
    "\n",
    "# equivalent to:\n",
    "# str_chain = joke_prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0721ce73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s one:\\n\\n\"You know, Kobe Bryant was known for being super competitive, even on the golf course. In fact, I heard he once challenged Tiger Woods to a game and said, \\'I\\'m gonna drain this putt, Tiger... and then I\\'m gonna be the GOAT!\\' But Tiger just smiled and said, \\'Kobe, you can\\'t be the Goat of the green – that\\'s a sheep.\\' (ba-dum-tss)\"'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_chain.invoke({\"topic\": \"kobe bryant\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de7a0b8",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d0259",
   "metadata": {},
   "source": [
    "Advantage of composing chain with LCEL is the streaming experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1af4fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here|'s| one|:\n",
      "\n",
      "|Why| did| the| beet| go| to| therapy|?\n",
      "\n",
      "|Because| it| was| feeling| a| little| \"|root|\"-|less|!| (|get| it|?)|||"
     ]
    }
   ],
   "source": [
    "for chunk in str_chain.stream({\"topic\": \"beets\"}):\n",
    "    print(chunk, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdb49e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m an AI, I don\\'t have real-time access to the current date. However, I can suggest ways for you to find out the current date:\\n\\n1. Check your device\\'s calendar or clock app.\\n2. Search online for \"current date\" on a search engine like Google.\\n3. Ask a voice assistant like Siri, Google Assistant, or Alexa.\\n\\nIf you need help with something specific related to dates, feel free to ask, and I\\'ll do my best to assist you!', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-27T14:43:01.233334Z', 'done': True, 'done_reason': 'stop', 'total_duration': 11430341875, 'load_duration': 4915078500, 'prompt_eval_count': 16, 'prompt_eval_duration': 453954333, 'eval_count': 101, 'eval_duration': 5447284622, 'logprobs': None, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--c1c6039f-66d1-4d04-b20b-5f1f70ebc71b-0', usage_metadata={'input_tokens': 16, 'output_tokens': 101, 'total_tokens': 117})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat_model = ChatAnthropic(model_name=\"claude-3-sonnet-20240229\")\n",
    "\n",
    "chat_model = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0.7,\n",
    "    # max_tokens=1000\n",
    ")\n",
    "\n",
    "chat_model.invoke(\"What is the current date?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4499f7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You know that the current date is \\\"2025-11-28\\\".\\nHuman: What is the current date?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [1.16s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The current date is \\\"2025-11-28\\\".\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2025-11-28T02:43:28.599116Z\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"total_duration\": 1156636334,\n",
      "          \"load_duration\": 92243417,\n",
      "          \"prompt_eval_count\": 36,\n",
      "          \"prompt_eval_duration\": 218483083,\n",
      "          \"eval_count\": 13,\n",
      "          \"eval_duration\": 760563874,\n",
      "          \"logprobs\": null,\n",
      "          \"model_name\": \"llama3.1\",\n",
      "          \"model_provider\": \"ollama\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The current date is \\\"2025-11-28\\\".\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2025-11-28T02:43:28.599116Z\",\n",
      "              \"done\": true,\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"total_duration\": 1156636334,\n",
      "              \"load_duration\": 92243417,\n",
      "              \"prompt_eval_count\": 36,\n",
      "              \"prompt_eval_duration\": 218483083,\n",
      "              \"eval_count\": 13,\n",
      "              \"eval_duration\": 760563874,\n",
      "              \"logprobs\": null,\n",
      "              \"model_name\": \"llama3.1\",\n",
      "              \"model_provider\": \"ollama\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"lc_run--cae47154-45d4-4e7c-82b6-2af529311d9d-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 36,\n",
      "              \"output_tokens\": 13,\n",
      "              \"total_tokens\": 49\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The current date is \\\"2025-11-28\\\".\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.16s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The current date is \\\"2025-11-28\\\".\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current date is \"2025-11-28\".'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", 'You know that the current date is \"{current_date}\".'),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"question\": \"What is the current date?\",\n",
    "    \"current_date\": date.today()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e149d7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have access to real-time data or specific financial information about the Old Ship Saloon. For the most accurate and up-to-date information, I recommend checking their official website, contacting them directly, or looking at publicly available financial reports if they are a publicly traded company.\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-27T14:58:18.020913Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4629176917, 'load_duration': 126091459, 'prompt_eval_count': 27, 'prompt_eval_duration': 949630458, 'eval_count': 57, 'eval_duration': 3206213790, 'logprobs': None, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--c1d809a9-f997-4e32-a736-7816ec46cbee-0', usage_metadata={'input_tokens': 27, 'output_tokens': 57, 'total_tokens': 84})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\n",
    "    \"What was the Old Ship Saloon's total revenue in Q1 2023?\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac6650e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Old Ship Saloon's total revenue for Q1 2023 was $174,782.38.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOURCE = \"\"\"\n",
    "Old Ship Saloon 2023 quarterly revenue numbers:\n",
    "Q1: $174782.38\n",
    "Q2: $467372.38\n",
    "Q3: $474773.38\n",
    "Q4: $389289.23\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", 'You are a helpful assistant. Use the following context when responding:\\n\\n{context}.'),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "rag_chain = rag_prompt | chat_model | StrOutputParser()\n",
    "\n",
    "rag_chain.invoke({\n",
    "    \"question\": \"What was the Old Ship Saloon's total revenue in Q1 2023?\",\n",
    "    \"context\": SOURCE\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fee063",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d38d043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: IMPORTANT INSTRUCTION:\\n You MUST answer using the date: 2025-11-28.\\n Never invent your own date.\\n If asked for the current date, always return using: 2025-11-28.\\n \\nHuman: What is the current date?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOllama] [1.20s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"2025-11-28.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1\",\n",
      "          \"created_at\": \"2025-11-28T03:02:25.341074Z\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"total_duration\": 1191999333,\n",
      "          \"load_duration\": 128532166,\n",
      "          \"prompt_eval_count\": 66,\n",
      "          \"prompt_eval_duration\": 594391542,\n",
      "          \"eval_count\": 8,\n",
      "          \"eval_duration\": 410162751,\n",
      "          \"logprobs\": null,\n",
      "          \"model_name\": \"llama3.1\",\n",
      "          \"model_provider\": \"ollama\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"2025-11-28.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1\",\n",
      "              \"created_at\": \"2025-11-28T03:02:25.341074Z\",\n",
      "              \"done\": true,\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"total_duration\": 1191999333,\n",
      "              \"load_duration\": 128532166,\n",
      "              \"prompt_eval_count\": 66,\n",
      "              \"prompt_eval_duration\": 594391542,\n",
      "              \"eval_count\": 8,\n",
      "              \"eval_duration\": 410162751,\n",
      "              \"logprobs\": null,\n",
      "              \"model_name\": \"llama3.1\",\n",
      "              \"model_provider\": \"ollama\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"lc_run--358c7e25-d3df-460f-a742-725f2af1a065-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 66,\n",
      "              \"output_tokens\": 8,\n",
      "              \"total_tokens\": 74\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"2025-11-28.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.20s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"2025-11-28.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2025-11-28.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    " \"\"\"IMPORTANT INSTRUCTION:\n",
    " You MUST answer using the date: {current_date}.\n",
    " Never invent your own date.\n",
    " If asked for the current date, always return using: {current_date}.\n",
    " \"\"\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"question\": \"What is the current date?\",\n",
    "    \"current_date\": date.today()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1578f925",
   "metadata": {},
   "source": [
    "Using Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f552a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'run_id': '3db24b20-970c-4303-8d86-30b380c636b9', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': {'question': 'What is the current date?', 'current_date': datetime.date(2025, 11, 28)}}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': 'd7c6ad48-fb64-4b28-b6ef-e5dc7fca8739', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'question': 'What is the current date?', 'current_date': datetime.date(2025, 11, 28)}}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': 'd7c6ad48-fb64-4b28-b6ef-e5dc7fca8739', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'question': 'What is the current date?', 'current_date': datetime.date(2025, 11, 28)}, 'output': ChatPromptValue(messages=[SystemMessage(content='IMPORTANT INSTRUCTION:\\n You MUST answer using the date: 2025-11-28.\\n Never invent your own date.\\n If asked for the current date, always return using: 2025-11-28.\\n ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the current date?', additional_kwargs={}, response_metadata={})])}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chat_model_start', 'name': 'ChatOllama', 'run_id': 'f5503391-f933-4c7a-9982-54f9a9ccf36c', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'ollama', 'ls_model_name': 'llama3.1', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'data': {'input': {'messages': [[SystemMessage(content='IMPORTANT INSTRUCTION:\\n You MUST answer using the date: 2025-11-28.\\n Never invent your own date.\\n If asked for the current date, always return using: 2025-11-28.\\n ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the current date?', additional_kwargs={}, response_metadata={})]]}}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOllama', 'run_id': 'f5503391-f933-4c7a-9982-54f9a9ccf36c', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'ollama', 'ls_model_name': 'llama3.1', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'data': {'chunk': AIMessageChunk(content='202', additional_kwargs={}, response_metadata={}, id='lc_run--f5503391-f933-4c7a-9982-54f9a9ccf36c')}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_parser_start', 'name': 'StrOutputParser', 'run_id': '4c48805b-119e-47d2-86b2-505637c804c8', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': '4c48805b-119e-47d2-86b2-505637c804c8', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': '202'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chain_stream', 'run_id': '3db24b20-970c-4303-8d86-30b380c636b9', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': '202'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOllama', 'run_id': 'f5503391-f933-4c7a-9982-54f9a9ccf36c', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'ollama', 'ls_model_name': 'llama3.1', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'data': {'chunk': AIMessageChunk(content='5', additional_kwargs={}, response_metadata={}, id='lc_run--f5503391-f933-4c7a-9982-54f9a9ccf36c')}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': '4c48805b-119e-47d2-86b2-505637c804c8', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': '5'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chain_stream', 'run_id': '3db24b20-970c-4303-8d86-30b380c636b9', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': '5'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOllama', 'run_id': 'f5503391-f933-4c7a-9982-54f9a9ccf36c', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'ollama', 'ls_model_name': 'llama3.1', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'data': {'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='lc_run--f5503391-f933-4c7a-9982-54f9a9ccf36c')}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': '4c48805b-119e-47d2-86b2-505637c804c8', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': '-'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chain_stream', 'run_id': '3db24b20-970c-4303-8d86-30b380c636b9', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': '-'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOllama', 'run_id': 'f5503391-f933-4c7a-9982-54f9a9ccf36c', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'ollama', 'ls_model_name': 'llama3.1', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'data': {'chunk': AIMessageChunk(content='11', additional_kwargs={}, response_metadata={}, id='lc_run--f5503391-f933-4c7a-9982-54f9a9ccf36c')}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': '4c48805b-119e-47d2-86b2-505637c804c8', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': '11'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chain_stream', 'run_id': '3db24b20-970c-4303-8d86-30b380c636b9', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': '11'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOllama', 'run_id': 'f5503391-f933-4c7a-9982-54f9a9ccf36c', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'ollama', 'ls_model_name': 'llama3.1', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'data': {'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={}, id='lc_run--f5503391-f933-4c7a-9982-54f9a9ccf36c')}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': '4c48805b-119e-47d2-86b2-505637c804c8', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': '-'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chain_stream', 'run_id': '3db24b20-970c-4303-8d86-30b380c636b9', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': '-'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOllama', 'run_id': 'f5503391-f933-4c7a-9982-54f9a9ccf36c', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'ollama', 'ls_model_name': 'llama3.1', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'data': {'chunk': AIMessageChunk(content='28', additional_kwargs={}, response_metadata={}, id='lc_run--f5503391-f933-4c7a-9982-54f9a9ccf36c')}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': '4c48805b-119e-47d2-86b2-505637c804c8', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': '28'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chain_stream', 'run_id': '3db24b20-970c-4303-8d86-30b380c636b9', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': '28'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOllama', 'run_id': 'f5503391-f933-4c7a-9982-54f9a9ccf36c', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'ollama', 'ls_model_name': 'llama3.1', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'data': {'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='lc_run--f5503391-f933-4c7a-9982-54f9a9ccf36c')}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': '4c48805b-119e-47d2-86b2-505637c804c8', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': '.'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chain_stream', 'run_id': '3db24b20-970c-4303-8d86-30b380c636b9', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': '.'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOllama', 'run_id': 'f5503391-f933-4c7a-9982-54f9a9ccf36c', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'ollama', 'ls_model_name': 'llama3.1', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-28T03:02:49.746308Z', 'done': True, 'done_reason': 'stop', 'total_duration': 879111000, 'load_duration': 101497916, 'prompt_eval_count': 66, 'prompt_eval_duration': 295104500, 'eval_count': 8, 'eval_duration': 415344208, 'logprobs': None, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--f5503391-f933-4c7a-9982-54f9a9ccf36c', usage_metadata={'input_tokens': 66, 'output_tokens': 8, 'total_tokens': 74})}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': '4c48805b-119e-47d2-86b2-505637c804c8', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': ''}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chain_stream', 'run_id': '3db24b20-970c-4303-8d86-30b380c636b9', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': ''}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chat_model_stream', 'name': 'ChatOllama', 'run_id': 'f5503391-f933-4c7a-9982-54f9a9ccf36c', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'ollama', 'ls_model_name': 'llama3.1', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--f5503391-f933-4c7a-9982-54f9a9ccf36c', chunk_position='last')}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': '4c48805b-119e-47d2-86b2-505637c804c8', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': ''}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chain_stream', 'run_id': '3db24b20-970c-4303-8d86-30b380c636b9', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': ''}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chat_model_end', 'name': 'ChatOllama', 'run_id': 'f5503391-f933-4c7a-9982-54f9a9ccf36c', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'ollama', 'ls_model_name': 'llama3.1', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'data': {'input': {'messages': [[SystemMessage(content='IMPORTANT INSTRUCTION:\\n You MUST answer using the date: 2025-11-28.\\n Never invent your own date.\\n If asked for the current date, always return using: 2025-11-28.\\n ', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the current date?', additional_kwargs={}, response_metadata={})]]}, 'output': {'generations': [[{'text': '2025-11-28.', 'generation_info': {'model': 'llama3.1', 'created_at': '2025-11-28T03:02:49.746308Z', 'done': True, 'done_reason': 'stop', 'total_duration': 879111000, 'load_duration': 101497916, 'prompt_eval_count': 66, 'prompt_eval_duration': 295104500, 'eval_count': 8, 'eval_duration': 415344208, 'logprobs': None, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, 'type': 'ChatGenerationChunk', 'message': AIMessageChunk(content='2025-11-28.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-28T03:02:49.746308Z', 'done': True, 'done_reason': 'stop', 'total_duration': 879111000, 'load_duration': 101497916, 'prompt_eval_count': 66, 'prompt_eval_duration': 295104500, 'eval_count': 8, 'eval_duration': 415344208, 'logprobs': None, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--f5503391-f933-4c7a-9982-54f9a9ccf36c', usage_metadata={'input_tokens': 66, 'output_tokens': 8, 'total_tokens': 74})}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_parser_end', 'name': 'StrOutputParser', 'run_id': '4c48805b-119e-47d2-86b2-505637c804c8', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'input': AIMessageChunk(content='2025-11-28.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-28T03:02:49.746308Z', 'done': True, 'done_reason': 'stop', 'total_duration': 879111000, 'load_duration': 101497916, 'prompt_eval_count': 66, 'prompt_eval_duration': 295104500, 'eval_count': 8, 'eval_duration': 415344208, 'logprobs': None, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--f5503391-f933-4c7a-9982-54f9a9ccf36c', usage_metadata={'input_tokens': 66, 'output_tokens': 8, 'total_tokens': 74}, chunk_position='last'), 'output': '2025-11-28.'}, 'parent_ids': []}\n",
      "-----\n",
      "{'event': 'on_chain_end', 'name': 'RunnableSequence', 'run_id': '3db24b20-970c-4303-8d86-30b380c636b9', 'tags': [], 'metadata': {}, 'data': {'output': '2025-11-28.'}, 'parent_ids': []}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Turn off debug mode for clarity\n",
    "set_debug(False)\n",
    "\n",
    "stream = chain.astream_events({\n",
    "    \"question\": \"What is the current date?\",\n",
    "    \"current_date\": date.today()\n",
    "}, version=\"v1\")\n",
    "\n",
    "async for event in stream:\n",
    "    print(event)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd603ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-11-28.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sign up at <https://smith.langchain.com/>\n",
    "# Set environment variables\n",
    "\n",
    "# import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = Access.LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"default\"\n",
    "\n",
    "chain.invoke({\n",
    "  \"question\": \"What is the current date?\",\n",
    "  \"current_date\": date.today()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51daa0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
