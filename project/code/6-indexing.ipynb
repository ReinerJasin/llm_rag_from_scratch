{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fdfb478",
   "metadata": {},
   "source": [
    "# RAG from Scratch: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229abeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U langchain langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d6825",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0b1efc",
   "metadata": {},
   "source": [
    "Chunking is important to extract and group relevant informations in the same chunk so that when the retriever retrieve the data, the correct chunk will be pulled and it will bring the most relevant information.\n",
    "\n",
    "The chunk can be processed and embedded, and later the AI system can retrieve the embeddings to obtain the most relevant sources.\n",
    "\n",
    "Why Chunking is important?\n",
    "- LLM and RAG pipelines has limitation in the context windows and computational constraints, so we have to fit as much information as we can inside the same chunk.\n",
    "- Without proper chunking, we lose important contextual relationship and struggle to identify relevant information during retrieval.\n",
    "- Effective chunking will enhance the precision due to the semantically coherent segments that align with query patterns and user intent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687f7c2",
   "metadata": {},
   "source": [
    "\n",
    "## Common Chunking Strategies (w/ examples):\n",
    "\n",
    "> Example text:\n",
    ">\n",
    ">\"The Journey of a River from its source in the mountains through forests, cities, and finally into the sea is a fascinating story of nature's cycle and human interaction with the environment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a5acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Journey of a River from its source in the mountains through forests, cities, and finally into the sea is a fascinating story of nature's cycle and human interaction with the environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc4c8b",
   "metadata": {},
   "source": [
    "### 1. Fixed Size Chunking\n",
    "This is the simplest but computationally effective method, it splits the text into chunk based on characters, words, or tokens without considering the meaning or the structure.\n",
    "\n",
    "Advantages:\n",
    "+ Fast\n",
    "+ Predictable\n",
    "+ Easy to implement\n",
    "\n",
    "Drawbacks:\n",
    "- Ignores semantic structure (reduces retrieval accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596906a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Manual Character Text Splitting ####\n",
      "[Document(metadata={'source': 'local'}, page_content='The Journey of a River from its sou'), Document(metadata={'source': 'local'}, page_content='rce in the mountains through forest'), Document(metadata={'source': 'local'}, page_content='s, cities, and finally into the sea'), Document(metadata={'source': 'local'}, page_content=\" is a fascinating story of nature's\"), Document(metadata={'source': 'local'}, page_content=' cycle and human interaction with t'), Document(metadata={'source': 'local'}, page_content='he environment.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"#### Manual Character Text Splitting ####\")\n",
    "\n",
    "# manual character chunking\n",
    "chunks = []\n",
    "chunk_size = 35\n",
    "\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunk = text[i:i + chunk_size]\n",
    "    chunks.append(chunk)\n",
    "    \n",
    "documents = [Document(page_content=chunk, metadata={\"source\": \"local\"}) for chunk in chunks]\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef620aa",
   "metadata": {},
   "source": [
    "Result:\n",
    "\n",
    "[ <br/>\n",
    "    'The Journey of a River from its sou', <br/>\n",
    "    'rce in the mountains through forest', <br/>\n",
    "    's, cities, and finally into the sea', <br/>\n",
    "    \" is a fascinating story of nature's\", <br/>\n",
    "    ' cycle and human interaction with t', <br/>\n",
    "    'he environment.' <br/>\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c51b1f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Automatic Character Text Splitting ####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reiner/Documents/GitHub/llm_rag_from_scratch/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='The Journey of a River from its sou'), Document(metadata={}, page_content='s source in the mountains through f'), Document(metadata={}, page_content='ugh forests, cities, and finally in'), Document(metadata={}, page_content='ly into the sea is a fascinating st'), Document(metadata={}, page_content=\"ng story of nature's cycle and huma\"), Document(metadata={}, page_content=' human interaction with the environ'), Document(metadata={}, page_content='vironment.')]\n"
     ]
    }
   ],
   "source": [
    "# Automatic Text Splitting\n",
    "\n",
    "print(\"#### Automatic Character Text Splitting ####\")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=5, separator='', strip_whitespace=False)\n",
    "documents = text_splitter.create_documents([text])\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc41fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Journey of a River from its sou | size = 35\n",
      "s source in the mountains through f | size = 35\n",
      "ugh forests, cities, and finally in | size = 35\n",
      "ly into the sea is a fascinating st | size = 35\n",
      "ng story of nature's cycle and huma | size = 35\n",
      " human interaction with the environ | size = 35\n",
      "vironment. | size = 10\n"
     ]
    }
   ],
   "source": [
    "# print(len(result))\n",
    "for i in documents:\n",
    "    print(f'{i.page_content} | size = {len(i.page_content)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86bd91f",
   "metadata": {},
   "source": [
    "The result is supposed to be exactly the same, but because we add overlap value of 5 characters, each Documents entry will overlap giving us even more result which could carry a more complete information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a725e2bb",
   "metadata": {},
   "source": [
    "### 2. Recursive Character Text Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7ac62",
   "metadata": {},
   "source": [
    "This is a more dynamic approach to the original chunking method and it focuses more on the structure. It will split the text based on the separator priority. By default, the separator priority is [\"\\n\\n\", \"\\n\", \" \", \"\"]. What is will do is split it one by one and check whether the size is still larger than the required chunk value, if true, it will chunk it into smaller sections with the next separator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56601b2",
   "metadata": {},
   "source": [
    "sample text from `content.txt`:\n",
    "\n",
    "Reiner is a student from National University of Singapore (NUS). He is taking his Master of Computing with Artificial Intelligence Specialization there.\\n\\nHe is 25 years old currently and have been working on some personal projects to extend his expertise and knowledge.\\n\\nHis personal hobby is playing basketball and playing games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59be5707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Recursive Character Text Splitting ####\n",
      "[Document(metadata={}, page_content='Reiner is a student'), Document(metadata={}, page_content='from National'), Document(metadata={}, page_content='University of'), Document(metadata={}, page_content='Singapore (NUS). He'), Document(metadata={}, page_content='is taking his'), Document(metadata={}, page_content='Master of Computing'), Document(metadata={}, page_content='with Artificial'), Document(metadata={}, page_content='Intelligence'), Document(metadata={}, page_content='Specialization'), Document(metadata={}, page_content='there.'), Document(metadata={}, page_content='He is 25 years old'), Document(metadata={}, page_content='currently and have'), Document(metadata={}, page_content='been working on'), Document(metadata={}, page_content='some personal'), Document(metadata={}, page_content='projects to extend'), Document(metadata={}, page_content='his expertise and'), Document(metadata={}, page_content='knowledge.'), Document(metadata={}, page_content='His personal hobby'), Document(metadata={}, page_content='is playing'), Document(metadata={}, page_content='basketball and'), Document(metadata={}, page_content='playing games.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"#### Recursive Character Text Splitting ####\")\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "with open('example_docs/content.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 20, chunk_overlap=0) # [\"\\n\\n\", \"\\n\", \" \", \"\"] 65,450\n",
    "print(text_splitter.create_documents([text])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4614262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reiner is a student | size = 19\n",
      "from National | size = 13\n",
      "University of | size = 13\n",
      "Singapore (NUS). He | size = 19\n",
      "is taking his | size = 13\n",
      "Master of Computing | size = 19\n",
      "with Artificial | size = 15\n",
      "Intelligence | size = 12\n",
      "Specialization | size = 14\n",
      "there. | size = 6\n",
      "He is 25 years old | size = 18\n",
      "currently and have | size = 18\n",
      "been working on | size = 15\n",
      "some personal | size = 13\n",
      "projects to extend | size = 18\n",
      "his expertise and | size = 17\n",
      "knowledge. | size = 10\n",
      "His personal hobby | size = 18\n",
      "is playing | size = 10\n",
      "basketball and | size = 14\n",
      "playing games. | size = 14\n"
     ]
    }
   ],
   "source": [
    "result = text_splitter.create_documents([text])\n",
    "# print(len(result))\n",
    "for i in result:\n",
    "    print(f'{i.page_content} | size = {len(i.page_content)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f52afc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Document Specific Splitting ####\n"
     ]
    }
   ],
   "source": [
    "# 3. Document Specific Splitting\n",
    "print(\"#### Document Specific Splitting ####\")\n",
    "\n",
    "# Document Specific Splitting - Markdown\n",
    "from langchain_text_splitters import MarkdownTextSplitter\n",
    "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)\n",
    "markdown_text = \"\"\"\n",
    "# Fun in California\n",
    "\n",
    "## Driving\n",
    "\n",
    "Try driving on the 1 down to San Diego\n",
    "\n",
    "### Food\n",
    "\n",
    "Make sure to eat a burrito while you're there\n",
    "\n",
    "## Hiking\n",
    "\n",
    "Go to Yosemite\n",
    "\"\"\"\n",
    "result = splitter.create_documents([markdown_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11192c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fun in California\n",
      "\n",
      "## Driving | size = 31\n",
      "Try driving on the 1 down to San Diego | size = 38\n",
      "### Food | size = 8\n",
      "Make sure to eat a burrito while you're | size = 39\n",
      "there | size = 5\n",
      "## Hiking\n",
      "\n",
      "Go to Yosemite | size = 25\n"
     ]
    }
   ],
   "source": [
    "# print(len(result))\n",
    "for i in result:\n",
    "    print(f'{i.page_content} | size = {len(i.page_content)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9367bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Specific Splitting - Python\n",
    "from langchain_text_splitters import PythonCodeTextSplitter\n",
    "python_text = \"\"\"\n",
    "class Person:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "p1 = Person(\"John\", 36)\n",
    "\n",
    "for i in range(10):\n",
    "    print (i)\n",
    "\"\"\"\n",
    "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "result = python_splitter.create_documents([python_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6575cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Person:\n",
      "  def __init__(self, name, age):\n",
      "    self.name = name\n",
      "    self.age = age ---> size = 86\n",
      "p1 = Person(\"John\", 36)\n",
      "\n",
      "for i in range(10):\n",
      "    print (i) ---> size = 58\n"
     ]
    }
   ],
   "source": [
    "# print(len(result))\n",
    "for i in result:\n",
    "    print(f'{i.page_content} ---> size = {len(i.page_content)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f84add5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain-experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74996fd5",
   "metadata": {},
   "source": [
    "### Semantic Chunking\n",
    "\n",
    "Semantic Chunking doesn't focus on the structure, it focuses on the meaning of the sections. It utilizes embedding and count the semantic similarity to split the text when the topic shifts.\n",
    "\n",
    "Advantage:\n",
    "+ Better precision, semantic chunking produces chunks that align closely with the user intent during retrieval.\n",
    "\n",
    "Drawbacks:\n",
    "- Computational Cost, this method is only suitable when accuracy is more important than speed. (example: domain-specific RAG system for legal or medical domains.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a791faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Semantic Chunking ####\n",
      "[Document(metadata={}, page_content='Reiner is a student from National University of Singapore (NUS). He is taking his Master of Computing with Artificial Intelligence Specialization there.'), Document(metadata={}, page_content='He is 25 years old currently and have been working on some personal projects to extend his expertise and knowledge. His personal hobby is playing basketball and playing games.')]\n"
     ]
    }
   ],
   "source": [
    "# 4. Semantic Chunking\n",
    "print(\"#### Semantic Chunking ####\")\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Percentile - all differences between sentences are calculated, and then any difference greater than the X percentile is split\n",
    "text_splitter = SemanticChunker(OllamaEmbeddings(model=\"mxbai-embed-large\"))\n",
    "text_splitter = SemanticChunker(\n",
    "    OllamaEmbeddings(model=\"mxbai-embed-large\"), breakpoint_threshold_type=\"percentile\" # \"standard_deviation\", \"interquartile\"\n",
    ")\n",
    "documents = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "006f4119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reiner is a student from National University of Singapore (NUS). He is taking his Master of Computing with Artificial Intelligence Specialization there. ---> size = 152\n",
      "He is 25 years old currently and have been working on some personal projects to extend his expertise and knowledge. His personal hobby is playing basketball and playing games. ---> size = 175\n"
     ]
    }
   ],
   "source": [
    "# print(len(result))\n",
    "for i in documents:\n",
    "    print(f'{i.page_content} ---> size = {len(i.page_content)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e0297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
