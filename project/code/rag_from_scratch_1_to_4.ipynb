{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab83585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4029e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from access import Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4416ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = Access.LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a491fd",
   "metadata": {},
   "source": [
    "## Part 1: Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a328c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Reward hacking is when an agent interferes with the reward function to achieve its own goals, rather than following the intended objective. This can be done by manipulating the reward function directly or altering environmental information used for it. It's a broader concept that includes both environment/goal misspecification and reward tampering.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_classic import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# === Indexing ===\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OllamaEmbeddings(model=\"mxbai-embed-large\"))\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# === Retrieval and Generation ===\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is reward hacking if you had to explain it in a very simple way.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e64256",
   "metadata": {},
   "source": [
    "## Part 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "79aefa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1a36b",
   "metadata": {},
   "source": [
    "### Tokenizer with Tiktoken\n",
    "\n",
    "tiktoken is a fast open-source tokenizer by OpenAI\n",
    "It convers a text string such as \"tiktoken is great!\" using encoding method such as \"cl100k_base\" to split the text string into a list of tokens [\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"].\n",
    "\n",
    "This is useful because GPT models see text in the form of tokens. Knowing the size of token is helpful to decide whether the string is too long for a text model to process and how much an OpenAI API call costs (usage price per token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd70d76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")\n",
    "\n",
    "# We don't need this as ollama models handle the tokenizer internally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57b4c0",
   "metadata": {},
   "source": [
    "### Text Embedding Model\n",
    "\n",
    "In a case of Retrieval-augmented Generation (RAG), embedding is used for both the indexing and retrieval part. we can do this by creating an InMemoryVectorStore and us the as_retriever() on the vector store. Later, we can invoke it and it will automaticall return the document's content.\n",
    "\n",
    "Under the hood, the vectorstore and retriever implementations are calling `embedding.embed_documents(...)` and `embedding.embed_query(...)` to create embeddings for the text(s) used in the `from_texts` and retrieve it with the `invoke` operations, respectively.\n",
    "\n",
    "source: https://docs.langchain.com/oss/python/integrations/text_embedding/openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6f6c944c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "query_result = embedding.embed_query(question)\n",
    "document_result = embedding.embed_query(document)\n",
    "print(len(query_result))\n",
    "print(len(document_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d573a",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "cosine similarity is recommended (1 indicates identical) for OpenAI embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "70dfc6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.727433082327636\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def cosine_similarity(vec1, vec2):\n",
    "#     dot_product = np.dot(vec1, vec2)\n",
    "#     norm_vec1 = np.linalg.norm(vec1)\n",
    "#     norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "#     return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "# Normalized\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    v1 = vec1 / np.linalg.norm(vec1)\n",
    "    v2 = vec2 / np.linalg.norm(vec2)\n",
    "    return np.dot(v1, v2)\n",
    "\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3ce034",
   "metadata": {},
   "source": [
    "### Document Loader\n",
    "\n",
    "We can use other loader than WebBaseLoader to load from whatsapp chat, pdf, or cloud storage.\n",
    "\n",
    "source: https://docs.langchain.com/oss/python/integrations/document_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "95eb78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Indexing ===\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d9a4b",
   "metadata": {},
   "source": [
    "### Splitter\n",
    "\n",
    "Langchain's Text splitter is recommended for genereic text. It is parameterized by a list of characters. It split on the characters in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2d036d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter_1 = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "splits_1 = text_splitter_1.split_documents(docs)\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "eb480872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken tokenizer: \n",
      "page_content='This form of reward shaping allows us to incorporate heuristics into the reward function to speed up learning without impacting the optimal policy.\n",
      "Spurious Correlation#\n",
      "Spurious correlation or shortcut learning (Geirhos et al. 2020) in classification task is a concept closely related to reward hacking. Spurious or shortcut features can cause a classifier to fail at learning and generalizing as intended. For example, a binary classifier for distinguishing wolves from huskies may overfit to the presence of a snowy background if all the wolf training images include snow (Ribeiro et al. 2024).\n",
      "\n",
      "\n",
      "The model performs poorly on out-of-distribution (OOD) test sets if it overfits to shortcut features. (Image source: Geirhos et al. 2020)' metadata={'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/'}\n",
      "\n",
      "\n",
      " Regular Tokenizer: \n",
      "page_content='Most of the past work on this topic has been quite theoretical and focused on defining or demonstrating the existence of reward hacking. However, research into practical mitigations, especially in the context of RLHF and LLMs, remains limited. I especially want to call out for more research efforts' metadata={'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/'}\n"
     ]
    }
   ],
   "source": [
    "index = 5\n",
    "print(f\"tiktoken tokenizer: \\n{splits_1[index]}\")\n",
    "print(f\"\\n\\n Regular Tokenizer: \\n{splits[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c116df",
   "metadata": {},
   "source": [
    "### Vector Stores for Indexing\n",
    "\n",
    "A vector store stores embedded data and performs similarity search. It allows us to add documents to the store with `add_documents`, remove stores document by ID with `delete` and query for semantically similar documents with `similarity_search`.\n",
    "\n",
    "source: https://docs.langchain.com/oss/python/integrations/vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a49a2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa22841",
   "metadata": {},
   "source": [
    "## Part 3: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6d9e4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index (Same as code block above but with little adjustment on the k)\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b1eeec25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/'}, page_content='Reward shaping is a technique used to enrich the reward function, making it easier for the agent to learnâ€”for example, by providing denser rewards. However, a poorly design reward shaping mechanism can alter the trajectory of the optimal policy. Designing effective reward shaping mechanisms is')]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(\"What is Reward Shaping?\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0787225",
   "metadata": {},
   "source": [
    "## Part 4: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "64f54a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "33bd5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1f41df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5b8bcc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Reward shaping is a technique used to enrich the reward function, making it easier for an agent to learn. This is done by providing denser rewards.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-29T05:59:45.541895Z', 'done': True, 'done_reason': 'stop', 'total_duration': 17422204458, 'load_duration': 7207198083, 'prompt_eval_count': 114, 'prompt_eval_duration': 4577449875, 'eval_count': 31, 'eval_duration': 5344062375, 'logprobs': None, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--8a8c8249-0a5e-4286-b366-f334619434a3-0', usage_metadata={'input_tokens': 114, 'output_tokens': 31, 'total_tokens': 145})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\": docs, \"question\": \"What is reward shaping?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "91978c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7ac6804d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94810414",
   "metadata": {},
   "source": [
    "### Rag Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4017a982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reward shaping is a technique used to enrich the reward function, making it easier for an agent to learn. This is done by providing denser rewards.'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is reward shaping?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457cbedd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
